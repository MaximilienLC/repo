SCALING LAW ANALYSIS: DL vs GA in Behavior Cloning
====================================================

EXPERIMENT 1: CartPole-v1 Behavior Cloning
-------------------------------------------

Setup:
- Environment: CartPole-v1 (4D state, 2 discrete actions)
- Expert: Trained via REINFORCE (500 episodes, avg return ~25)
- Dataset: 18,771 state-action pairs from expert rollouts
- Test set: 5,000 state-action pairs
- Dataset sizes: [100, 300, 1000, 3000, 10000]

Methods:
- DL: 2-layer MLP (64â†’32 hidden units), Adam optimizer (lr=1e-3), cross-entropy loss, 150 epochs
- GA: Same architecture, population=100, generations=100, tournament selection (k=3), mutation_rate=0.01

Metrics:
1. Action match rate (can saturate at 100%)
2. Average episode return

Results:
- Action match rate: Both DL and GA achieved 100% across ALL dataset sizes (even with just 100 samples)
- Episode return: Both achieved ~9.3-9.4 (expert level: 9.39)

Conclusion:
TASK TOO SIMPLE. Both methods saturate perfectly. No differential saturation behavior observed.

Next Steps:
Need to increase complexity per proposal directive ("work our way up to more complexity if needs be"):
1. More complex environment (e.g., LunarLander, MountainCar continuous)
2. More complex expert behavior (suboptimal expert, multi-modal policies)
3. More challenging objective (continuous control, partial observability)
4. Reduced data quality (noisy demonstrations, distribution shift)
5. Architectural constraints (smaller networks to limit capacity)

Files Generated:
- dl_ga_scaling_experiment.py: Main experimental code
- dl_vs_ga_scaling.png: Scaling curves visualization
- scaling_results.pkl: Raw numerical results

Key Implementation Notes:
- Expert trained poorly (avg return ~25 vs CartPole max ~500) leading to trivial cloning task
- Both methods overparameterized for task complexity
- Need harder task where gradient-based optimization constraints (differentiability, data hyperdependency) create observable limitations
